{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RAG Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=cdiD-9MMpb0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Elon Musk is a billionaire entrepreneur and CEO of multiple companies, including Tesla, SpaceX, Neuralink, and The Boring Company. He is known for his ambitious vision for the future, including the colonization of Mars, the development of sustainable energy solutions, and the advancement of artificial intelligence. Musk is also known for his outspoken and sometimes controversial statements on social media.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 73, 'prompt_tokens': 12, 'total_tokens': 85, 'completion_tokens_details': {'audio_tokens': 0, 'reasoning_tokens': 0, 'accepted_prediction_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-aa057e9d-45a7-49c5-b5e2-9f373ad0d083-0', usage_metadata={'input_tokens': 12, 'output_tokens': 73, 'total_tokens': 85})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0)\n",
    "\n",
    "# Test the model\n",
    "model.invoke(\"Who is Elon Musk?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Elon Musk is a billionaire entrepreneur and CEO of multiple companies, including Tesla, SpaceX, Neuralink, and The Boring Company. He is known for his work in the fields of electric vehicles, space exploration, and renewable energy. Musk is also a prominent figure in popular culture and is often referred to as a visionary and innovator.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's use StrOutputParser to extract the answer as a sstring\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()\n",
    "chain = model | parser\n",
    "chain.invoke(\"Who is Elon Musk?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduce Prompts Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Human: \\nAnswer the question based on the context below. If you can\\'t answer the question, \\nreply \"My apologies, but I have no clue\".env\\nContext: Britney\\'s sister is Alyssia\\n\\nQuestion: Is Alyssia\\'s sister?\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't answer the question, \n",
    "reply \"My apologies, but I have no clue\".env\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "mycontext = \"Britney's sister is Alyssia\"\n",
    "myquestion = \"Is Alyssia's sister?\"\n",
    "prompt.format(context = mycontext, question=myquestion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's chain the prompt with the model and output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Yes, Britney is Alyssia's sister.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | parser\n",
    "chain.invoke({\n",
    "    \"context\" :  mycontext, \n",
    "    \"question\" : myquestion\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate / combining Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a new prompt template for translating the output into Spanish/ French\n",
    "translation_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Translate the {answer} to {language}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spanish: Sarra tiene una hermana, Cerine.\\nPortuguese: Sarra tem uma irmã, Cerine.\\nFrench: Sarra a une sœur, Cerine.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's create a new translation chain that combines the first chain with the second one (translation prompt)\n",
    "from operator import itemgetter\n",
    "translation_chain = (\n",
    "    {\"answer\": chain, \"language\": itemgetter(\"language\")} | translation_prompt | model | parser\n",
    ")\n",
    "translation_chain.invoke(\n",
    "    {\n",
    "        \"context\": \"Sarra's sister is Cerine. She does not have any more siblings.\",\n",
    "        \"question\": \"How many sisters does Sarra have?\",\n",
    "        \"language\": [\"Spanish\", \"Portuguese\", \"French\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcribing the YouTube Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We want to send a context to the model from YouTUBE. Let's OpenAI Whisper\n",
    "# import tempfile\n",
    "# import whisper\n",
    "# from pytubefix import YouTube\n",
    "\n",
    "# YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=u47GtXwePms\"\n",
    "# # Check if file not exist then create ...\n",
    "# if not os.path.exists(\"transcription.txt\"):\n",
    "#     youtube = YouTube(YOUTUBE_VIDEO)\n",
    "#     audio = youtube.streams.filter(only_audio=True).first()\n",
    "    \n",
    "#     # Let's loas the base model. Not accurate\n",
    "#     whisper_model = whisper.load_model(\"base\")\n",
    "    \n",
    "#     with tempfile.TemporaryDirectory() as tmpdir:\n",
    "#         file = audio.download(output_path=tmpdir)\n",
    "#         transcription = whisper_model.transcribe(file, fp16=False)[\"text\"].strip()\n",
    "        \n",
    "#         with open(\"transcription.txt\", \"w\") as file:\n",
    "#             file.write(transcription)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's read the transcription and display the first few characters to ensure all is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Let's talk about RAG versus fine-tuning. Now, they're both powerful ways to enhance the capabilities of large language m\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"transcription.txt\") as file:\n",
    "    transcription = file.read()\n",
    "transcription[: 120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the entire transcription as context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some challenges of LLM include limitations in providing accurate or up-to-date information for specific queries, being very generalistic in nature, and the need to specialize them for specific use cases and adapt them in enterprise applications.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    response = chain.invoke({\n",
    "        \"context\": transcription,\n",
    "        \"question\": \"What are some challenges of LLM? \"\n",
    "    })\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the transcript in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'transcription.txt'}, page_content=\"Let's talk about RAG versus fine-tuning. Now, they're both powerful ways to enhance the capabilities of large language models, but today you're going to learn about their strengths, their use cases, and how you can choose between them. So one of the biggest issues with dealing with generative AI right now is one, enhancing the models, but also two, dealing with their limitations. For example, I just recently asked my favorite LLM a simple question, who won the Euro 2024 World Championship? And while this might seem like a simple query for my model, well, there's a slight issue. Because the model wasn't trained on that specific information, it can't give me an accurate or up-to-date answer. At the same time, these popular models are very generalistic. And so how do we think about specializing them for specific use cases and adapt them in enterprise applications? Because your data is one of the most important things that you can work with. And in the field of AI, using techniques such as RAG or fine-tuning will allow you to supercharge the capabilities that your application delivers. So in the next few minutes, we're going to learn about both of these techniques, the differences between them, and where you can start seeing and using them in. Let's get started. So let's begin with retrieval augmented generation, which is a way to increase the capabilities of a model through retrieving external and up-to-date information, augmenting the original prompt that was given to the model, and then generating a response back using that context and information. And this is really powerful, because if we think back about that example of with the EuroCup, well, the model didn't have the information and context to provide an answer. And this is one of the big limitations of LLMs, but this is mitigated in a way with RAG, because now, instead of having an incorrect or possibly a hallucinated answer, we're able to work with what's known as a corpus of information. So this could be data, this could be PDFs, documents, spreadsheets, things that are relevant to our specific organization or knowledge that we need to specialize in. So when the query comes in this time, we're working with what's known as a retriever that's able to pull the correct documents and relative context to what the question is, and then pass that knowledge, as well as the original prompt to a large language model. And with its intuition and pre-trained data, it's able to give us a response back based on that contextualized information, which is really, really powerful, because we can start to see that we can get better responses back from a model with our proprietary and confidential information without needing to do any retraining on the model. And this is a great and popular way to enhance the capabilities of a model without having to do any fine tuning. So as the name implies, what this involves is taking a large language foundational model, but this time we're going to be specializing it in a certain domain or area. So we're working with labeled and targeted data that's going to be provided to the model. And when we do some processing, we'll have a specialized model for a specific use case to talk in a certain style, to have a certain tone that could represent our organization or company. And so then when a model is queried from a user or any other type of way, we'll have a response that gives the correct tone and output or specialty in a domain that we'd like to receive. And this is really important because what we're doing is essentially baking in this context and intuition into the model. And it's really important because this is now a part of the model's weights versus being supplemented on top with a technique like RAG. Okay, so we understand how both of these techniques can enhance a model's accuracy, output, and performance. But let's take a look at their strengths and weaknesses and some common use cases because the direction that you go in can greatly affect a model's performance, its accuracy, outputs, compute costs, and much, much more. So let's begin with retrieval augmented generation. And something that I want to point out here is that because we're working with a corpus of information and data, this is perfect for dynamic data sources such as databases and other data repositories where we want to continuously pull information and have that up to date for the model to use and understand. And at the same time, because we're working with this retriever system and passing in the information as context in the prompt, well, that really helps with hallucinations. And providing the sources for this information is really important in systems where we need trust and transparency when we're using AI. So this is fantastic. But let's also think about this whole system because having this efficient retrieval system is really important in how we select and pick the data that we want to provide in that limited context window. And so maintaining this is also something that you need to think about. And at the same time, what we're doing here in this system is effectively supplementing that information on top of the model. So we're not essentially enhancing the base model itself, we're just giving it the relative and contextual information it needs. Versus fine tuning is a little bit different because we're actually baking in that context and intuition into the model while we have greater influence in essentially how the model behaves and reacts in different situations. Is it an insurance adjuster? Can it summarize documents? Whatever we want the model to do, we can essentially use fine tuning in order to help with that process. And at the same time, because that is baked into the model's weights itself, well, that's really great for speed and inference costs and a variety of other factors that come to running models. So for example, we can use smaller prompt context windows, in order to get the responses that we want from the model. And as we begin to specialize these models, they can get smaller and smaller for specific use case. So it's really great for running these specific specialized models in a variety of use cases. But at the same time, we have the same issue of cutoff. So up until the point where the model is trained, well, after that, we have no more additional information that we can give to the model. So the same issue that we had with the World Cup example. So both of these have their strengths and weaknesses. But let's actually see this in some examples and use cases here. So when you're thinking about choosing between RAG and fine tuning, it's really important to consider your AI-enabled application's priorities and requirements. So mainly, this starts off with the data. Is the data that you're working with slow moving, or is it fast? For example, if we need to use up-to-date external information and have that ready contextually every time we use a model, then this could be a great use case for RAG. For example, a product documentation chatbot where we can continually update the responses with up-to-date information. Now, at the same time, let's think about the industry that you might be in. Now, fine tuning is really powerful for specific industries that have nuances in their writing styles, terminology, vocabulary. And so, for example, if we have a legal document summarizer, well, this could be a perfect use case for fine tuning. Now, let's think about sources. This is really important right now in having transparency behind our models. And with RAG being able to provide the context and where the information came from is really, really great. And so this could be a great use case, again, for that chatbot, for retail, insurance, and a variety of other specialties where having that source and information in the context of the prompt is very important. But at the same time, we may have things such as past data in our organization that we can use to train a model. So let it be accustomed to the data that we're going to be working with. For example, again, that legal summarizer could have past data on different legal cases and documents that we feed it so that it understands the situation that it's working in and we have better, more desirable outputs. So this is cool, but I think the best situation is a combination of both of these methods. So let's say we have a financial news reporting service. Well, we could fine tune it to be native to the industry of finance and understand all the lingo there. We could also give it past data of financial records and let it understand how we work in that specific industry, but also be able to provide the most up-to-date sources for news and data and be able to provide that with a level of confidence and transparency and trust to the end user who's making that decision and needs to know the source. And this is really where a combination of fine tuning and RAG is so awesome because we can really build amazing applications, taking advantage of both RAG as a way to retrieve that information and have it up to date, but fine tuning to specialize our data, but also specialize our model in a certain domain. So they're both wonderful techniques and they have their strengths, but the choice to use one or a combination of both techniques is up to you and your specific use case and data. So thank you so much for watching. As always, if you have any questions about fine tuning, RAG, or all AI related topics, let us know in the comment section below. Don't forget to like the video and subscribe to the channel for more content. Thanks so much for watching.\")]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"transcription.txt\")\n",
    "text_documents = loader.load()\n",
    "text_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split text into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'transcription.txt'}, page_content=\"Let's talk about RAG versus fine-tuning. Now, they're both powerful ways to enhance the capabilities of large language models, but today you're going to learn about their strengths, their use cases, and how you can choose between them. So one of the biggest issues with dealing with generative AI right now is one, enhancing the models, but also two, dealing with their limitations. For example, I just recently asked my favorite LLM a simple question, who won the Euro 2024 World Championship? And while this might seem like a simple query for my model, well, there's a slight issue. Because the model wasn't trained on that specific information, it can't give me an accurate or up-to-date answer. At the same time, these popular models are very generalistic. And so how do we think about specializing them for specific use cases and adapt them in enterprise applications? Because your data is one of the most important things that you can work with. And in the field of AI, using techniques such as\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"them for specific use cases and adapt them in enterprise applications? Because your data is one of the most important things that you can work with. And in the field of AI, using techniques such as RAG or fine-tuning will allow you to supercharge the capabilities that your application delivers. So in the next few minutes, we're going to learn about both of these techniques, the differences between them, and where you can start seeing and using them in. Let's get started. So let's begin with retrieval augmented generation, which is a way to increase the capabilities of a model through retrieving external and up-to-date information, augmenting the original prompt that was given to the model, and then generating a response back using that context and information. And this is really powerful, because if we think back about that example of with the EuroCup, well, the model didn't have the information and context to provide an answer. And this is one of the big limitations of LLMs, but this\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"because if we think back about that example of with the EuroCup, well, the model didn't have the information and context to provide an answer. And this is one of the big limitations of LLMs, but this is mitigated in a way with RAG, because now, instead of having an incorrect or possibly a hallucinated answer, we're able to work with what's known as a corpus of information. So this could be data, this could be PDFs, documents, spreadsheets, things that are relevant to our specific organization or knowledge that we need to specialize in. So when the query comes in this time, we're working with what's known as a retriever that's able to pull the correct documents and relative context to what the question is, and then pass that knowledge, as well as the original prompt to a large language model. And with its intuition and pre-trained data, it's able to give us a response back based on that contextualized information, which is really, really powerful, because we can start to see that we\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"And with its intuition and pre-trained data, it's able to give us a response back based on that contextualized information, which is really, really powerful, because we can start to see that we can get better responses back from a model with our proprietary and confidential information without needing to do any retraining on the model. And this is a great and popular way to enhance the capabilities of a model without having to do any fine tuning. So as the name implies, what this involves is taking a large language foundational model, but this time we're going to be specializing it in a certain domain or area. So we're working with labeled and targeted data that's going to be provided to the model. And when we do some processing, we'll have a specialized model for a specific use case to talk in a certain style, to have a certain tone that could represent our organization or company. And so then when a model is queried from a user or any other type of way, we'll have a response that\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"talk in a certain style, to have a certain tone that could represent our organization or company. And so then when a model is queried from a user or any other type of way, we'll have a response that gives the correct tone and output or specialty in a domain that we'd like to receive. And this is really important because what we're doing is essentially baking in this context and intuition into the model. And it's really important because this is now a part of the model's weights versus being supplemented on top with a technique like RAG. Okay, so we understand how both of these techniques can enhance a model's accuracy, output, and performance. But let's take a look at their strengths and weaknesses and some common use cases because the direction that you go in can greatly affect a model's performance, its accuracy, outputs, compute costs, and much, much more. So let's begin with retrieval augmented generation. And something that I want to point out here is that because we're working\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"performance, its accuracy, outputs, compute costs, and much, much more. So let's begin with retrieval augmented generation. And something that I want to point out here is that because we're working with a corpus of information and data, this is perfect for dynamic data sources such as databases and other data repositories where we want to continuously pull information and have that up to date for the model to use and understand. And at the same time, because we're working with this retriever system and passing in the information as context in the prompt, well, that really helps with hallucinations. And providing the sources for this information is really important in systems where we need trust and transparency when we're using AI. So this is fantastic. But let's also think about this whole system because having this efficient retrieval system is really important in how we select and pick the data that we want to provide in that limited context window. And so maintaining this is also\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"system because having this efficient retrieval system is really important in how we select and pick the data that we want to provide in that limited context window. And so maintaining this is also something that you need to think about. And at the same time, what we're doing here in this system is effectively supplementing that information on top of the model. So we're not essentially enhancing the base model itself, we're just giving it the relative and contextual information it needs. Versus fine tuning is a little bit different because we're actually baking in that context and intuition into the model while we have greater influence in essentially how the model behaves and reacts in different situations. Is it an insurance adjuster? Can it summarize documents? Whatever we want the model to do, we can essentially use fine tuning in order to help with that process. And at the same time, because that is baked into the model's weights itself, well, that's really great for speed and\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"to do, we can essentially use fine tuning in order to help with that process. And at the same time, because that is baked into the model's weights itself, well, that's really great for speed and inference costs and a variety of other factors that come to running models. So for example, we can use smaller prompt context windows, in order to get the responses that we want from the model. And as we begin to specialize these models, they can get smaller and smaller for specific use case. So it's really great for running these specific specialized models in a variety of use cases. But at the same time, we have the same issue of cutoff. So up until the point where the model is trained, well, after that, we have no more additional information that we can give to the model. So the same issue that we had with the World Cup example. So both of these have their strengths and weaknesses. But let's actually see this in some examples and use cases here. So when you're thinking about choosing\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"that we had with the World Cup example. So both of these have their strengths and weaknesses. But let's actually see this in some examples and use cases here. So when you're thinking about choosing between RAG and fine tuning, it's really important to consider your AI-enabled application's priorities and requirements. So mainly, this starts off with the data. Is the data that you're working with slow moving, or is it fast? For example, if we need to use up-to-date external information and have that ready contextually every time we use a model, then this could be a great use case for RAG. For example, a product documentation chatbot where we can continually update the responses with up-to-date information. Now, at the same time, let's think about the industry that you might be in. Now, fine tuning is really powerful for specific industries that have nuances in their writing styles, terminology, vocabulary. And so, for example, if we have a legal document summarizer, well, this could be\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"tuning is really powerful for specific industries that have nuances in their writing styles, terminology, vocabulary. And so, for example, if we have a legal document summarizer, well, this could be a perfect use case for fine tuning. Now, let's think about sources. This is really important right now in having transparency behind our models. And with RAG being able to provide the context and where the information came from is really, really great. And so this could be a great use case, again, for that chatbot, for retail, insurance, and a variety of other specialties where having that source and information in the context of the prompt is very important. But at the same time, we may have things such as past data in our organization that we can use to train a model. So let it be accustomed to the data that we're going to be working with. For example, again, that legal summarizer could have past data on different legal cases and documents that we feed it so that it understands the\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"to the data that we're going to be working with. For example, again, that legal summarizer could have past data on different legal cases and documents that we feed it so that it understands the situation that it's working in and we have better, more desirable outputs. So this is cool, but I think the best situation is a combination of both of these methods. So let's say we have a financial news reporting service. Well, we could fine tune it to be native to the industry of finance and understand all the lingo there. We could also give it past data of financial records and let it understand how we work in that specific industry, but also be able to provide the most up-to-date sources for news and data and be able to provide that with a level of confidence and transparency and trust to the end user who's making that decision and needs to know the source. And this is really where a combination of fine tuning and RAG is so awesome because we can really build amazing applications, taking\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"end user who's making that decision and needs to know the source. And this is really where a combination of fine tuning and RAG is so awesome because we can really build amazing applications, taking advantage of both RAG as a way to retrieve that information and have it up to date, but fine tuning to specialize our data, but also specialize our model in a certain domain. So they're both wonderful techniques and they have their strengths, but the choice to use one or a combination of both techniques is up to you and your specific use case and data. So thank you so much for watching. As always, if you have any questions about fine tuning, RAG, or all AI related topics, let us know in the comment section below. Don't forget to like the video and subscribe to the channel for more content. Thanks so much for watching.\")]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "text_splitter.split_documents(text_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the 'most' relevant chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded length: 1536\n",
      "[-0.0021738149225711823, -0.008245505392551422, -0.007408461533486843, -0.008813945576548576, -0.006939966697245836, 0.017240602523088455, -0.0003338024253025651, 0.013417685404419899, 0.004094643052667379, -0.022512728348374367, 0.00396346440538764, -0.012892971746623516, 0.01120014488697052, -0.010675430297851562, 0.006149772554636002, -0.02646057680249214, 0.025136297568678856, -0.008645287714898586, 0.02948392741382122, -0.026935316622257233]\n"
     ]
    }
   ],
   "source": [
    "# Generate the embeddings for an arbitrary query\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "embedded_query = embeddings.embed_query(\"Who is Britney's sister?\")\n",
    "print(f\"Embedded length: {len(embedded_query)}\")\n",
    "print(embedded_query[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embedding of two sample contexts\n",
    "context1 = embeddings.embed_query(\"Britney's sister is Alyssia\")\n",
    "context2 = embeddings.embed_query(\"Hatim's mother is a lecturer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarities: (0.9266278574518928, 0.7321724059856504)\n"
     ]
    }
   ],
   "source": [
    "# Let's use Cosine Similarity to compute the similarity between the query and both contexts\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "query_context1_similarity = cosine_similarity([embedded_query], [context1])[0][0]\n",
    "query_context2_similarity = cosine_similarity([embedded_query], [context2])[0][0]\n",
    "print(f\"Similarities: {query_context1_similarity, query_context2_similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up a Knowledge Base (KB) / Vector Store (VS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "A KB/VS is a database consisting of embeddings that specializes in fast similarity searches\n",
    "\n",
    "'''\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "vectorstore1 = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"Britney's sister is Alyssia\",\n",
    "        \"Kamil's beloved is Angelina Jolie\",\n",
    "        \"Steve and Bill are brothers\",\n",
    "        \"Susu likes blue cars\",\n",
    "        \"hatim's mother is a lecturer\",\n",
    "        \"Aziz drives Ferrari\",\n",
    "        \"Newton has two siblings\"\n",
    "    ],embedding=embeddings\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Query the vector store to find and retrieve similar embeddings to a given query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(page_content='Newton has two siblings'), 0.7401043216058089),\n",
       " (Document(page_content='Steve and Bill are brothers'), 0.736737873485554),\n",
       " (Document(page_content=\"Kamil's beloved is Angelina Jolie\"),\n",
       "  0.7177979498989019),\n",
       " (Document(page_content=\"hatim's mother is a lecturer\"), 0.716337677717846),\n",
       " (Document(page_content=\"Britney's sister is Alyssia\"), 0.7068967830519695)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the world population?\"\n",
    "vectorstore1.similarity_search_with_score(query = query, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting the KB / VS to the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Newton has two siblings'),\n",
       " Document(page_content='Steve and Bill are brothers'),\n",
       " Document(page_content=\"Britney's sister is Alyssia\"),\n",
       " Document(page_content=\"hatim's mother is a lecturer\")]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "We can use the vector store to store the transcription.txt and retrieve relevant chunks from the latter\n",
    "and send them to the model afterwards.\n",
    "- TODO:\n",
    "    - Configure a Retriever: will run similarity search in the VS and return the most similar chunks\n",
    "    - We can get a retriever directly from the vector store\n",
    "'''\n",
    "chunks_retriever = vectorstore1.as_retriever()\n",
    "chunks_retriever.invoke(\"How many siblings does Newton have?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reminder: \n",
    "\n",
    "**Our prompt expects two parameters, to with, \"context\" and \"question\".\n",
    "We can use the retriever to find the relevant chunks we will use as the context to answer\n",
    " the question.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': [Document(page_content='Newton has two siblings'),\n",
       "  Document(page_content='Steve and Bill are brothers'),\n",
       "  Document(page_content=\"Britney's sister is Alyssia\"),\n",
       "  Document(page_content=\"hatim's mother is a lecturer\")],\n",
       " 'question': 'How many siblings does Newtom have?'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "setup = RunnableParallel(context = chunks_retriever, question=RunnablePassthrough())\n",
    "setup.invoke(\"How many siblings does Newtom have?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add the setup map to the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Newton has two siblings.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = setup | prompt | model | parser\n",
    "chain.invoke(\"How many siblings does Newtom have?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aziz drives a Ferrari.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"What car does Aziz drive?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Kamil's beloved is Angelina Jolie.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Whose beloved is Angelina Jolie?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Transcript into the Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the chain using the correct vectorstore.add()\n",
    "documents = text_splitter.split_documents(text_documents)\n",
    "trans_vectorstore = DocArrayInMemorySearch.from_documents(documents=documents, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG stands for retrieval augmented generation, which is a technique used to increase the capabilities of a model by retrieving external and up-to-date information, augmenting the original prompt given to the model, and generating a response using that context and information.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (\n",
    "    {\"context\": trans_vectorstore.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    |parser\n",
    ")\n",
    "chain.invoke(\"What is RAG?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use Pinecone as Vector Store.\n",
    "Pinecone is the leading AI infrastructure for building accurate, secure, and scalable AI applications. Use Pinecone Database to store and search vector data at scale, or start with Pinecone Assistant to get a RAG application running in minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "pinecone_index_name = \"rcwcourses2024\"\n",
    "\n",
    "pinecone = PineconeVectorStore.from_documents(\n",
    "    documents=documents, embedding=embeddings, index_name=pinecone_index_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Pinecone as retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'transcription.txt'}, page_content=\"that we had with the World Cup example. So both of these have their strengths and weaknesses. But let's actually see this in some examples and use cases here. So when you're thinking about choosing between RAG and fine tuning, it's really important to consider your AI-enabled application's priorities and requirements. So mainly, this starts off with the data. Is the data that you're working with slow moving, or is it fast? For example, if we need to use up-to-date external information and have that ready contextually every time we use a model, then this could be a great use case for RAG. For example, a product documentation chatbot where we can continually update the responses with up-to-date information. Now, at the same time, let's think about the industry that you might be in. Now, fine tuning is really powerful for specific industries that have nuances in their writing styles, terminology, vocabulary. And so, for example, if we have a legal document summarizer, well, this could be\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"end user who's making that decision and needs to know the source. And this is really where a combination of fine tuning and RAG is so awesome because we can really build amazing applications, taking advantage of both RAG as a way to retrieve that information and have it up to date, but fine tuning to specialize our data, but also specialize our model in a certain domain. So they're both wonderful techniques and they have their strengths, but the choice to use one or a combination of both techniques is up to you and your specific use case and data. So thank you so much for watching. As always, if you have any questions about fine tuning, RAG, or all AI related topics, let us know in the comment section below. Don't forget to like the video and subscribe to the channel for more content. Thanks so much for watching.\"),\n",
       " Document(metadata={'source': 'transcription.txt'}, page_content=\"them for specific use cases and adapt them in enterprise applications? Because your data is one of the most important things that you can work with. And in the field of AI, using techniques such as RAG or fine-tuning will allow you to supercharge the capabilities that your application delivers. So in the next few minutes, we're going to learn about both of these techniques, the differences between them, and where you can start seeing and using them in. Let's get started. So let's begin with retrieval augmented generation, which is a way to increase the capabilities of a model through retrieving external and up-to-date information, augmenting the original prompt that was given to the model, and then generating a response back using that context and information. And this is really powerful, because if we think back about that example of with the EuroCup, well, the model didn't have the information and context to provide an answer. And this is one of the big limitations of LLMs, but this\")]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pinecone.similarity_search(\"How does RAG differ from Fine-Tune?\")[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Picone with the Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG (Retrieval Augmented Generation) enhances the capabilities of a model by retrieving external and up-to-date information, augmenting the original prompt given to the model, and generating a response using that context and information. On the other hand, Fine-Tuning is a technique that specializes the model in a certain domain by adjusting its parameters based on specific data from that domain.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (\n",
    "    {\"context\": pinecone.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n",
    "chain.invoke(\"How does RAG differ from Fine-Tune?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
